{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBRjIglq5OtzGY1oHg570U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gowrish21S/DL_assignment/blob/main/TIC_tac_Toe_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqXl9fJGGsjs",
        "outputId": "3669275a-d709-408f-8ec2-dc2ac5332efd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training...\n",
            "Rounds 0 | P1 Exp Rate: 1.0000\n",
            "Rounds 1000 | P1 Exp Rate: 0.9048\n",
            "Rounds 2000 | P1 Exp Rate: 0.8187\n",
            "Rounds 3000 | P1 Exp Rate: 0.7408\n",
            "Rounds 4000 | P1 Exp Rate: 0.6703\n",
            "Rounds 5000 | P1 Exp Rate: 0.6065\n",
            "Rounds 6000 | P1 Exp Rate: 0.5488\n",
            "Rounds 7000 | P1 Exp Rate: 0.4966\n",
            "Rounds 8000 | P1 Exp Rate: 0.4493\n",
            "Rounds 9000 | P1 Exp Rate: 0.4066\n",
            "Rounds 10000 | P1 Exp Rate: 0.3679\n",
            "Rounds 11000 | P1 Exp Rate: 0.3329\n",
            "Rounds 12000 | P1 Exp Rate: 0.3012\n",
            "Rounds 13000 | P1 Exp Rate: 0.2725\n",
            "Rounds 14000 | P1 Exp Rate: 0.2466\n",
            "Rounds 15000 | P1 Exp Rate: 0.2231\n",
            "Rounds 16000 | P1 Exp Rate: 0.2019\n",
            "Rounds 17000 | P1 Exp Rate: 0.1827\n",
            "Rounds 18000 | P1 Exp Rate: 0.1653\n",
            "Rounds 19000 | P1 Exp Rate: 0.1496\n",
            "Rounds 20000 | P1 Exp Rate: 0.1353\n",
            "Rounds 21000 | P1 Exp Rate: 0.1224\n",
            "Rounds 22000 | P1 Exp Rate: 0.1108\n",
            "Rounds 23000 | P1 Exp Rate: 0.1002\n",
            "Rounds 24000 | P1 Exp Rate: 0.0907\n",
            "Rounds 25000 | P1 Exp Rate: 0.0821\n",
            "Rounds 26000 | P1 Exp Rate: 0.0743\n",
            "Rounds 27000 | P1 Exp Rate: 0.0672\n",
            "Rounds 28000 | P1 Exp Rate: 0.0608\n",
            "Rounds 29000 | P1 Exp Rate: 0.0550\n",
            "Rounds 30000 | P1 Exp Rate: 0.0498\n",
            "Rounds 31000 | P1 Exp Rate: 0.0450\n",
            "Rounds 32000 | P1 Exp Rate: 0.0408\n",
            "Rounds 33000 | P1 Exp Rate: 0.0369\n",
            "Rounds 34000 | P1 Exp Rate: 0.0334\n",
            "Rounds 35000 | P1 Exp Rate: 0.0302\n",
            "Rounds 36000 | P1 Exp Rate: 0.0273\n",
            "Rounds 37000 | P1 Exp Rate: 0.0247\n",
            "Rounds 38000 | P1 Exp Rate: 0.0224\n",
            "Rounds 39000 | P1 Exp Rate: 0.0202\n",
            "Rounds 40000 | P1 Exp Rate: 0.0183\n",
            "Rounds 41000 | P1 Exp Rate: 0.0166\n",
            "Rounds 42000 | P1 Exp Rate: 0.0150\n",
            "Rounds 43000 | P1 Exp Rate: 0.0136\n",
            "Rounds 44000 | P1 Exp Rate: 0.0123\n",
            "Rounds 45000 | P1 Exp Rate: 0.0111\n",
            "Rounds 46000 | P1 Exp Rate: 0.0100\n",
            "Rounds 47000 | P1 Exp Rate: 0.0100\n",
            "Rounds 48000 | P1 Exp Rate: 0.0100\n",
            "Rounds 49000 | P1 Exp Rate: 0.0100\n",
            "-------------\n",
            "| x |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "Input your action row (0-2):2\n",
            "Input your action col (0-2):1\n",
            "-------------\n",
            "| x |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "-------------\n",
            "| x | x |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "Input your action row (0-2):0\n",
            "Input your action col (0-2):3\n",
            "Invalid position. Try again.\n",
            "Input your action row (0-2):0\n",
            "Input your action col (0-2):2\n",
            "-------------\n",
            "| x | x | o | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "-------------\n",
            "| x | x | o | \n",
            "-------------\n",
            "| x |   |   | \n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "Input your action row (0-2):1\n",
            "Input your action col (0-2):1\n",
            "-------------\n",
            "| x | x | o | \n",
            "-------------\n",
            "| x | o |   | \n",
            "-------------\n",
            "|   | o |   | \n",
            "-------------\n",
            "-------------\n",
            "| x | x | o | \n",
            "-------------\n",
            "| x | o |   | \n",
            "-------------\n",
            "| x | o |   | \n",
            "-------------\n",
            "computer wins!\n",
            "---\n",
            "If you wish to continue press 'y', else press 'n'\n",
            "Input your choice to continue:n\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "BOARD_ROWS = 3\n",
        "BOARD_COLS = 3\n",
        "\n",
        "\n",
        "# --- State Class (No Change from previous Q-Learning version) ---\n",
        "class State:\n",
        "    def __init__(self, p1, p2):\n",
        "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "        self.p1 = p1\n",
        "        self.p2 = p2\n",
        "        self.isEnd = False\n",
        "        self.boardHash = None\n",
        "        self.playerSymbol = 1\n",
        "        self.current_player = p1\n",
        "\n",
        "    def getHash(self):\n",
        "        self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS))\n",
        "        return self.boardHash\n",
        "\n",
        "    def winner(self):\n",
        "        # row\n",
        "        for i in range(BOARD_ROWS):\n",
        "            if sum(self.board[i, :]) == 3:\n",
        "                self.isEnd = True\n",
        "                return 1\n",
        "            if sum(self.board[i, :]) == -3:\n",
        "                self.isEnd = True\n",
        "                return -1\n",
        "        # col\n",
        "        for i in range(BOARD_COLS):\n",
        "            if sum(self.board[:, i]) == 3:\n",
        "                self.isEnd = True\n",
        "                return 1\n",
        "            if sum(self.board[:, i]) == -3:\n",
        "                self.isEnd = True\n",
        "                return -1\n",
        "        # diagonal\n",
        "        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
        "        diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)])\n",
        "\n",
        "        if abs(diag_sum1) == 3 or abs(diag_sum2) == 3:\n",
        "            self.isEnd = True\n",
        "            if diag_sum1 == 3 or diag_sum2 == 3:\n",
        "                return 1\n",
        "            else:\n",
        "                return -1\n",
        "\n",
        "        # tie\n",
        "        if len(self.availablePositions()) == 0:\n",
        "            self.isEnd = True\n",
        "            return 0\n",
        "\n",
        "        # not end\n",
        "        self.isEnd = False\n",
        "        return None\n",
        "\n",
        "    def availablePositions(self):\n",
        "        positions = []\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                if self.board[i, j] == 0:\n",
        "                    positions.append((i, j))\n",
        "        return positions\n",
        "\n",
        "    def updateState(self, position):\n",
        "        self.board[position] = self.playerSymbol\n",
        "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
        "        self.current_player = self.p2 if self.playerSymbol == -1 else self.p1\n",
        "\n",
        "    def giveReward(self, winner):\n",
        "        if winner == 1:\n",
        "            return 1\n",
        "        elif winner == -1:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0.5\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "        self.boardHash = None\n",
        "        self.isEnd = False\n",
        "        self.playerSymbol = 1\n",
        "        self.current_player = self.p1\n",
        "\n",
        "    def play(self, rounds=100, exp_decay_rate=0.9999):\n",
        "        for i in range(rounds):\n",
        "            if i % 1000 == 0:\n",
        "                print(\"Rounds {} | P1 Exp Rate: {:.4f}\".format(i, self.p1.exp_rate))\n",
        "\n",
        "            self.p1.update_exp_rate(exp_decay_rate)\n",
        "            self.p2.update_exp_rate(exp_decay_rate)\n",
        "\n",
        "            self.p1.reset_history()\n",
        "            self.p2.reset_history()\n",
        "            self.reset() # Reset the board for a new game\n",
        "\n",
        "            while not self.isEnd:\n",
        "                player_making_move = self.current_player # The player whose turn it is\n",
        "                player_making_move_symbol = self.playerSymbol # Symbol of player making move\n",
        "\n",
        "                positions_before_move = self.availablePositions()\n",
        "                if not positions_before_move: # Should ideally be caught by winner() tie\n",
        "                    self.isEnd = True\n",
        "                    continue\n",
        "\n",
        "                # Player chooses an action\n",
        "                action = player_making_move.chooseAction(positions_before_move, self.board, player_making_move_symbol)\n",
        "\n",
        "                # Store the state and action *before* the move is made, in the player's history\n",
        "                player_making_move.addState(self.getHash(), action)\n",
        "\n",
        "                # Make the move on the board\n",
        "                self.updateState(action) # This internally flips self.playerSymbol and self.current_player\n",
        "\n",
        "                # Check for game end after the move\n",
        "                win = self.winner()\n",
        "\n",
        "                # Get the hash and available positions for the *next* state (after the move)\n",
        "                next_state_hash = self.getHash()\n",
        "                next_state_positions = self.availablePositions()\n",
        "\n",
        "                if win is not None:\n",
        "                    # Game ended. The player who just moved (player_making_move) receives the reward.\n",
        "                    reward_for_player = 0\n",
        "                    if win == player_making_move_symbol: # The player who just moved won\n",
        "                        reward_for_player = 1\n",
        "                    elif win == 0: # It's a tie\n",
        "                        reward_for_player = 0.5\n",
        "                    # If player_making_move lost (win == -player_making_move_symbol), reward remains 0.\n",
        "\n",
        "                    # Update the Q-table for the player who just moved (terminal state, so next_max_q_value is 0)\n",
        "                    player_making_move.updateQTable(reward_for_player, next_max_q_value=0)\n",
        "\n",
        "                    # Although not strictly part of this bug fix, in a more complete implementation,\n",
        "                    # the opponent's last move (if any) might also need to be updated with terminal rewards.\n",
        "                    # For simplicity and focusing on the IndexError, we primarily update the current mover.\n",
        "\n",
        "                    self.reset() # Reset the board for the next round\n",
        "                    break\n",
        "\n",
        "                else:\n",
        "                    # Game continues. Reward for the player who just moved is 0 for non-terminal states.\n",
        "                    # Calculate the maximum expected Q-value for the *next* state (s') from the perspective\n",
        "                    # of the player who just moved (player_making_move) using their own Q-table.\n",
        "                    next_max_q_value_for_update = player_making_move.get_max_q_for_state(next_state_hash, next_state_positions)\n",
        "\n",
        "                    # Update the Q-table for the player who just moved\n",
        "                    player_making_move.updateQTable(0, next_max_q_value_for_update)\n",
        "\n",
        "    def play2(self):\n",
        "        while not self.isEnd:\n",
        "            # Player 1 (AI)\n",
        "            positions = self.availablePositions()\n",
        "            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
        "\n",
        "            self.updateState(p1_action)\n",
        "            self.showBoard()\n",
        "\n",
        "            win = self.winner()\n",
        "            if win is not None:\n",
        "                if win == 1:\n",
        "                    print(self.p1.name, \"wins!\")\n",
        "                elif win == 0:\n",
        "                    print(\"tie!\")\n",
        "                else:\n",
        "                    print(self.p2.name, \"wins!\")\n",
        "                self.reset()\n",
        "                break\n",
        "\n",
        "            # Player 2 (Human)\n",
        "            positions = self.availablePositions()\n",
        "            p2_action = self.p2.chooseAction(positions)\n",
        "\n",
        "            self.updateState(p2_action)\n",
        "            self.showBoard()\n",
        "\n",
        "            win = self.winner()\n",
        "            if win is not None:\n",
        "                if win == -1:\n",
        "                    print(self.p2.name, \"wins!\")\n",
        "                else:\n",
        "                    print(\"tie!\")\n",
        "                self.reset()\n",
        "                break\n",
        "\n",
        "    def showBoard(self):\n",
        "        for i in range(0, BOARD_ROWS):\n",
        "            print('-------------')\n",
        "            out = '| '\n",
        "            for j in range(0, BOARD_COLS):\n",
        "                if self.board[i, j] == 1:\n",
        "                    token = 'x'\n",
        "                elif self.board[i, j] == -1:\n",
        "                    token = 'o'\n",
        "                else:\n",
        "                    token = ' '\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-------------')\n",
        "\n",
        "\n",
        "# --- QLearningPlayer Class (Modified) ---\n",
        "class QLearningPlayer:\n",
        "    def __init__(self, name, symbol, exp_rate=1.0, exp_min=0.01):\n",
        "        self.name = name\n",
        "        self.symbol = symbol\n",
        "        self.history = []\n",
        "        self.lr = 0.2\n",
        "        self.exp_rate = exp_rate\n",
        "        self.exp_min = exp_min\n",
        "        self.decay_gamma = 0.9\n",
        "        self.q_table = {}\n",
        "\n",
        "    def getHash(self, board):\n",
        "        return str(board.reshape(BOARD_COLS * BOARD_ROWS))\n",
        "\n",
        "    def chooseAction(self, positions, current_board, symbol):\n",
        "        current_hash = self.getHash(current_board)\n",
        "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
        "            # Exploration: choose a random action\n",
        "            idx = np.random.choice(len(positions))\n",
        "            action = positions[idx]\n",
        "        else:\n",
        "            # Exploitation: choose action with max Q-value\n",
        "            value_max = -np.inf # Initialize with negative infinity\n",
        "            action = None\n",
        "\n",
        "            for p in positions:\n",
        "                q_value = self.q_table.get((current_hash, p), 0)\n",
        "                if q_value > value_max:\n",
        "                    value_max = q_value\n",
        "                    action = p\n",
        "\n",
        "            # Fallback for when all Q-values are 0 or less (e.g., initial state) or no positions\n",
        "            if action is None and positions:\n",
        "                idx = np.random.choice(len(positions))\n",
        "                action = positions[idx]\n",
        "            elif action is None and not positions: # If no positions are available, this case shouldn't be reached ideally\n",
        "                # This indicates an issue upstream or a game end condition not fully handled\n",
        "                raise ValueError(\"No available positions for action selection.\")\n",
        "\n",
        "        return action\n",
        "\n",
        "    def addState(self, state_hash, action): # Removed next_q_value parameter\n",
        "        # Stores the state and action taken by this player\n",
        "        self.history.append({'state_hash': state_hash, 'action': action})\n",
        "\n",
        "    def updateQTable(self, reward, next_max_q_value): # Renamed for clarity: next_q_value is max(Q(s',a'))\n",
        "        st_act_info = self.history.pop() # Retrieve the last state-action pair from history\n",
        "        state_action = (st_act_info['state_hash'], st_act_info['action'])\n",
        "        old_q = self.q_table.get(state_action, 0)\n",
        "\n",
        "        # Q-Learning Update Rule: Q(s,a) = Q(s,a) + lr * [R + decay_gamma * max(Q(s',a')) - Q(s,a)]\n",
        "        new_q = old_q + self.lr * (reward + self.decay_gamma * next_max_q_value - old_q)\n",
        "        self.q_table[state_action] = new_q\n",
        "\n",
        "    # --- NEW METHOD: Get max Q-value for a given state ---\n",
        "    def get_max_q_for_state(self, state_hash, available_positions):\n",
        "        if not available_positions:\n",
        "            return 0 # If no moves are possible from this state, its value is 0 (terminal or no moves left)\n",
        "        max_q = -np.inf\n",
        "        for p in available_positions:\n",
        "            q_val = self.q_table.get((state_hash, p), 0)\n",
        "            if q_val > max_q:\n",
        "                max_q = q_val\n",
        "        return max_q if max_q != -np.inf else 0 # Return 0 if all Q-values are -inf (e.g., brand new state)\n",
        "\n",
        "    def update_exp_rate(self, decay_rate):\n",
        "        self.exp_rate = max(self.exp_min, self.exp_rate * decay_rate)\n",
        "\n",
        "    def reset_history(self):\n",
        "        self.history = []\n",
        "\n",
        "    def savePolicy(self):\n",
        "        fw = open('q_policy_' + str(self.name), 'wb')\n",
        "        pickle.dump(self.q_table, fw)\n",
        "        fw.close()\n",
        "\n",
        "    def loadPolicy(self, file):\n",
        "        fr = open(file, 'rb')\n",
        "        self.q_table = pickle.load(fr)\n",
        "        fr.close()\n",
        "\n",
        "\n",
        "class HumanPlayer:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def chooseAction(self, positions):\n",
        "        while True:\n",
        "            try:\n",
        "                row = int(input(\"Input your action row (0-2):\"))\n",
        "                col = int(input(\"Input your action col (0-2):\"))\n",
        "                action = (row, col)\n",
        "                if action in positions:\n",
        "                    return action\n",
        "                else:\n",
        "                    print(\"Invalid position. Try again.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter a number.\")\n",
        "            except Exception:\n",
        "                print(\"An error occurred.\")\n",
        "\n",
        "    def reset_history(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # training\n",
        "    # Start with exp_rate = 1.0 (100% exploration)\n",
        "    p1 = QLearningPlayer(\"p1\", symbol=1, exp_rate=1.0)\n",
        "    p2 = QLearningPlayer(\"p2\", symbol=-1, exp_rate=1.0)\n",
        "\n",
        "    st = State(p1, p2)\n",
        "    print(\"training...\")\n",
        "    # Use a small decay rate over many rounds to slowly reduce exploration\n",
        "    # Example: 1.0 * (0.9999)^50000 approx 0.0067 (just above the min of 0.01)\n",
        "    st.play(50000, exp_decay_rate=0.9999)\n",
        "\n",
        "    p1.savePolicy()\n",
        "    p2.savePolicy()\n",
        "\n",
        "    # play with human\n",
        "    # Set exp_rate=0 for deterministic, optimal play against the human\n",
        "    p1 = QLearningPlayer(\"computer\", symbol=1, exp_rate=0)\n",
        "    try:\n",
        "        p1.loadPolicy(\"q_policy_p1\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Policy file not found. Run training first.\")\n",
        "\n",
        "    p2 = HumanPlayer(\"human\")\n",
        "\n",
        "    st = State(p1, p2)\n",
        "\n",
        "    c ='y'\n",
        "    while (c == 'y'):\n",
        "        st.play2()\n",
        "        print(\"---\")\n",
        "        print(\"If you wish to continue press 'y', else press 'n'\")\n",
        "        c = (input(\"Input your choice to continue:\"))\n"
      ]
    }
  ]
}